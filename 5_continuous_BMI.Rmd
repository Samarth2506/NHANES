```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
rm(list = ls())
if(T){
  library(rnhanesdata)
  library(tidyverse)
  library(magrittr)
  library(data.table)
  library(randomForest)
  library(caret)
  library(keras)
  library(tensorflow)
}
```



```{r}
load(file = "analyticData.rda")
analyticData = analyticData %>% select(-permth_exm)
# NA: alive
# 1: deceased
analyticData$mortstat = ifelse(analyticData$mortstat %>% is.na,1,0)
# 1: alive, 0 : deceased
if(F){
  pca = prcomp(analyticData %>% select(-SEQN,-mortstat) %>% na.omit() ,
             center = T,
             scale. = T)
}
if(F){
  save(pca,file = 'pca.rda')
}
load(file  = 'pca.rda')
screeplot(pca)
if(F){
  pcscore = data.frame(SEQN = analyticData %>% na.omit %>% select(SEQN),
                       pca$x,
                       mortstat = analyticData %>% na.omit %>% select(mortstat))
}

if(F){
  save(pcscore,file = 'pcscore.rda')
}

load(file = 'pcscore.rda')
```


## Logistic regression on mortality (first 5 PCs)
```{r}
# first 5 PCs
y = pcscore[,c(2:6,which(colnames(pcscore) == 'mortstat'))]
y$mortstat = as.factor(y$mortstat)
set.seed(100)
# trainIdx = sample(c(TRUE, FALSE), dim(y)[1], replace = TRUE, prob = c(.7, .3))
trainIdx = sample(dim(y)[1],0.7*dim(y)[1])
fit = glm(mortstat ~ ., family = "binomial", data = y, subset = trainIdx)
summary(fit)

yPred =  (predict(fit, y[-trainIdx,], type = "response") > 0.5) * 1
ytest = y[-trainIdx, ]
ptab = table(ytest[,"mortstat"], yPred %>% factor(levels = levels(ytest[,"mortstat"] )))
# result
ptab
# acc
sum(diag(ptab)) / sum(ptab)
```



## BMI prediction based on PCscores
```{r}
load(file = 'pcscore.rda')

pcscore = pcscore %>% select(-mortstat) %>% 
  inner_join(Covariate_D %>% select(SEQN,BMI),by = "SEQN")

y = pcscore %>% select(-SEQN)
BMI = y$BMI 
y = y %>% select(-BMI) %>% mutate('log' = log(BMI+1))

ysub = y[,c(1:10,which(colnames(y) == 'log'))]
# first 10 PCs
fit = lm( log ~., data = ysub, subset = trainIdx)
# summary(fit)
yPred = exp(predict(fit,ysub[-trainIdx,]))-1
result = cbind(yPred,yTrue = BMI[-trainIdx]) %>% na.omit() %>% as.data.frame()

# MSE of yPred and yTrue
mean((result[,1]-result[,2])^2)

# visualization
library(ggplot2)
ggplot(data=result , aes(x = 1:dim(result)[1])) + 
  geom_line(aes(y = yPred),color = 'red') + 
  geom_line(aes(y = yTrue),color = 'blue') + 
  labs(title  = "First 10 PCs included") 
```

## Problem here: sync?
```{r}
ysub = y[,c(1:500,which(colnames(y) == 'log'))]
fit = lm( log ~., data = ysub, subset = trainIdx)
# summary(fit)
yPred = exp(predict(fit,ysub[-trainIdx,]))-1
result = cbind(yPred,yTrue = BMI[-trainIdx]) %>% na.omit() %>% as.data.frame()


# MSE of yPred and yTrue
mean((result[,1]-result[,2])^2)

# visualization
library(ggplot2)
ggplot(data=result , aes(x = 1:dim(result)[1])) + 
  geom_line(aes(y = yPred),color = 'red') + 
  geom_line(aes(y = yTrue),color = 'blue') + 
  labs(title  = "First 500 PCs included")
```
## Ridge regression first 500 PC scores
```{r}
set.seed(100)
trainIdx = sample(dim(analyticData)[1],0.8*dim(analyticData)[1])

ytrain = ysub$`log`[trainIdx]
xtrain = ysub[trainIdx,] %>% select(-log) %>% as.matrix()


library(glmnet)
fit <- glmnet(xtrain, ytrain, lambda = 
                cv.glmnet(xtrain, ytrain)$lambda.min)
ypred2 = predict(fit, newx = ysub[-trainIdx,] %>% select(-log) %>% as.matrix(),
                s = 'lambda.min')
yPred = (exp(ypred2)-1)
result = data.frame(yPred,yTrue = BMI[-trainIdx]) %>% na.omit()

mean((result[,1]-result[,2])^2)

library(ggplot2)
ggplot(data=result , aes(x = 1:dim(result)[1])) + 
  geom_line(aes(y = yPred),color = 'red') + 
  geom_line(aes(y = yTrue),color = 'blue')
```



## Brute Force lm( raw time series data ) same prob: sync?
```{r}
rm(list = ls())
load(file = "analyticData.rda")
analyticData = analyticData %>% select(-mortstat,-permth_exm) %>%
  inner_join(Covariate_D[,c('SEQN','BMI')],by = "SEQN") %>% 
  select(-SEQN) %>% na.omit()
BMI = analyticData$BMI
analyticData = analyticData %>% mutate('log' = log(BMI+1)) %>% select(-BMI)

set.seed(100)
trainIdx = sample(dim(analyticData)[1],0.8*dim(analyticData)[1])
y = analyticData
fit = lm( log~., data = y, subset = trainIdx)
# summary(fit)
yPred = exp(predict(fit,y[-trainIdx,]))-1
result = cbind(yPred,yTrue = BMI[-trainIdx]) %>% na.omit() %>% as.data.frame()
mean((result[,1]-result[,2])^2)
library(ggplot2)
ggplot(data=result , aes(x = 1:dim(result)[1])) + 
  geom_line(aes(y = yPred),color = 'red') + 
  geom_line(aes(y = yTrue),color = 'blue')
```

## Regularization: raw time series data
```{r}
ytrain = analyticData$`log`[trainIdx]
xtrain = analyticData[trainIdx,grep("MIN",colnames(analyticData))] %>% as.matrix()


library(glmnet)
fit <- glmnet(xtrain, ytrain, lambda = 
                cv.glmnet(xtrain, ytrain)$lambda.min)
ypred2 = predict(fit, newx = analyticData[-trainIdx,grep("MIN",colnames(analyticData))] %>% as.matrix(),
                s = 'lambda.min')
yPred = (exp(ypred2)-1)
result = data.frame(yPred,yTrue = BMI[-trainIdx]) %>% na.omit()

mean((result[,1]-result[,2])^2)

library(ggplot2)
ggplot(data=result , aes(x = 1:dim(result)[1])) + 
  geom_line(aes(y = yPred),color = 'red') + 
  geom_line(aes(y = yTrue),color = 'blue')

```


