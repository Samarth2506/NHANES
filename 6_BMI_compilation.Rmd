

```{r include=F,warning=FALSE,message=FALSE}
# rm(list = ls())

library(rnhanesdata)
library(tidyverse)
library(magrittr)
library(data.table)
library(randomForest)
library(caret)
library(keras)
library(tensorflow)
library(caTools)
library(mgcv)
library(boot)
library(car)
library(sjPlot)
library(gridExtra)
# library(PCAtools)

# https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/PAXRAW_D.htm
```




# pre processing
```{r include=F}
rm(list = ls())
keep_inx <- exclude_accel(act = PAXINTEN_D, flags = Flags_D)
accel_good_D <- PAXINTEN_D[keep_inx,] 
flags_good_D <- Flags_D[keep_inx,]


MIN_name = grep('MIN',colnames(accel_good_D),value = T)
MINdata = data.frame(SEQN = accel_good_D[,'SEQN'],accel_good_D[,MIN_name] * flags_good_D[,MIN_name]) %>%
  group_by(SEQN) %>% summarise_each(funs(mean)) %>% as.data.frame() %>% na.omit()


SEQN_7days = data.frame(SEQN = accel_good_D[,'SEQN'],accel_good_D[,MIN_name] * flags_good_D[,MIN_name]) %>% group_by(SEQN) %>% summarise(num = n()) %>% filter(num==7) %>% select(SEQN)
MINdata = MINdata %>% filter(SEQN %in% SEQN_7days$SEQN)

if(F){
  pca_model = prcomp(MINdata %>% select(-SEQN),
                     center = T,
                     scale. = T)
  save(pca_model,file= 'pca_model.rda')
}
load(file = 'pca_model.rda')

# summary(pca_model)

if(F){
eigs <- pca_model$sdev^2
cbind(
  SD = sqrt(eigs),
  Proportion = eigs/sum(eigs),
  Cumulative = cumsum(eigs)/sum(eigs))
}




analyticData = data.frame(SEQN=MINdata$SEQN,
                          pca_model$x) %>% 
  inner_join(Covariate_D %>% select(SEQN,Race,Gender,RIDAGEYR,BMI),by = 'SEQN') %>% 
  na.omit() 

analyticData = analyticData %>% filter(5 <BMI & BMI< 50)



save(MIN_name,MINdata,analyticData,file = 'analyticData.rda')

```




# PCA

```{r}
rm(list = ls())
load(file = 'analyticData.rda')
Y = analyticData
PCnames = paste('PC',1:5,sep = '')
y = analyticData[,c('RIDAGEYR','Race','Gender','BMI',PCnames)] %>% na.omit()
colnames(y)
if(F){
fit = randomForest(BMI ~ ., data = y,ntree = 1500,
                   importance = T)
save(fit,file = 'rffit.rda')
}
load(file = 'rffit.rda')
library(randomForest)
varImpPlot(fit,
           main = 'Variable Importance Measured by Random Forest')

# importance(fit)

y_cor <- as.data.frame(
  lapply(y, function (x) if (is.factor(x)) unclass(x) %>% as.numeric  else x))

library(ggcorrplot)
ggcorrplot(cor(y_cor), 
           lab = T,
           tl.cex = 15,
           title = "Pearsonâ€™s Correlation Analysis between \n BMI, Covariates and Physical Activity") + 
  labs(caption = 'Note. RIDAGEYR - Age in years at screening.') + 
  theme(plot.title = element_text(size = 20),
        plot.caption = element_text(size = 15))

```


# PCA results visualization
```{r}
library(gridExtra)
load(file = 'pca_model.rda')
temp = pca_model$rotation %>% data.frame()

gs = lapply(1:8, function(i) {
  ggplot(temp) + 
    aes(x = 1:nrow(temp)/60,y = unlist(temp[,i])) + 
    geom_line() +
    labs(x = "Time of Day", y = paste0("Loadings on PC",i)) +
    theme(axis.title.y = element_text(size = rel(0.9)))
})

# grid.arrange(grobs = gs,nrow = 3)
do.call(grid.arrange,c(gs,nrow = 3))

library(factoextra)
fviz_eig(pca_model)



# PC scores visualization
# https://stackoverflow.com/questions/27354734/dplyr-mutate-rowsums-calculations-or-custom-functions
# 
# temp = pca_model$x %>% as.data.frame()
# # colMeans(temp)
# 
# score = rbind(temp, data.frame(t(colMeans(temp))))
# 
# # find subject with highest / lowest PC scores
# score_sum = mutate(score,S = rowSums(score))
# # which(score_sum$S == max(score_sum$S))
# # subject 1931 has the highest average of scores
# # 1521 with lowest scores
# 
# temp = t(score[c(nrow(score), 1931, 1521),]) %>% as.data.frame()
# 
# n = 100
# tmp = temp[1:n,]
# ggplot(tmp, aes(x = 1:n)) + 
#   geom_line(aes(y = tmp[,1], color = "Average of Principal Component Scores")) + 
#   geom_line(aes(y = tmp[,2], color = "Participant with Highest Average of Principal Component scores")) +
#   geom_line(aes(y = tmp[,3], color = 'Participant with Lowest Average of Principal Component scores')) + 
#   labs(x = "Principal Component", y = "Principal Component Scores") +
#   scale_colour_manual(name = 'Legend', values = c('blue','red','green'))



  # geom_line(aes(x = 1:nrow(score)/60,
  #               y = temp[which(score$score_sum == max(score$score_sum)),]))

# g1 = ggplot(data.frame(res_RMSE)) + 
#   aes(x = 1:length(res_RMSE), y = res_RMSE) +
#   geom_line(aes(color = 'Original RMSE Loss'))+
#   # geom_smooth(se = TRUE, method = 'lm')+
#   geom_smooth(se = TRUE, method =  'auto', aes(color = 'LOESS Curve Fitting')) +
#   labs(x = 'Number of Principal Components Used in Cross-Validation',y = 'Root Mean Square Error ') + 
#   scale_colour_manual(name="Legend", values = c("blue",'red'))


```



# Cross-validation
```{r}
rm(list = ls())
load(file = 'analyticData.rda')


res_RMSE = c()
res_Rsquared = c()
if(F){
for(i in 1:100){
  PCnames = paste('PC',1:i,sep = '')
  train.control <- trainControl(method = "cv", number = 10)
  dat = analyticData[,c('RIDAGEYR','Race','Gender','BMI',PCnames)]
  model <- caret::train(BMI ~., data = dat, method = "lm",
               trControl = train.control)
  res_RMSE = c(res_RMSE, model$results$RMSE)
  res_Rsquared = c(res_Rsquared, model$results$Rsquared)
 
}
  save(res_RMSE,res_Rsquared, file = 'CV_100PCs.rda')
}


rm(list = ls())
load(file = 'analyticData.rda')

res_RMSE = c()
res_Rsquared = c()
if(F){
for(i in 1:50){
  PCnames = paste('PC',1:i,sep = '')
  train.control <- trainControl(method = "cv", number = 10)
  dat = analyticData[,c('RIDAGEYR','Race','Gender','BMI',PCnames)]
  model <- caret::train(BMI ~., data = dat, method = "lm",
               trControl = train.control)
  res_RMSE = c(res_RMSE, model$results$RMSE)
  res_Rsquared = c(res_Rsquared, model$results$Rsquared)
  
}
  save(res_RMSE,res_Rsquared, file = 'CV_50PCs.rda')
}

load(file = 'CV_50PCs.rda')
g1 = ggplot(data.frame(res_RMSE)) + 
  aes(x = 1:length(res_RMSE), y = res_RMSE) +
  geom_line(aes(color = 'Original RMSE Loss'))+
  # geom_smooth(se = TRUE, method = 'lm')+
  geom_smooth(se = TRUE, method =  'auto', aes(color = 'LOESS Curve Fitting')) +
  labs(x = 'Number of Principal Components Used in Cross-Validation',y = 'Root Mean Square Error ') + 
  scale_colour_manual(name="Legend", values = c("blue",'red'))

g2 = ggplot(data.frame(res_Rsquared)) + 
  aes(x = 1:length(res_Rsquared), y = res_Rsquared) +
  geom_line(aes(color = 'Original R-squared'))+
  # geom_smooth(se = TRUE, method = 'lm')+
  geom_smooth(se = TRUE, method =  'auto', aes(color = 'LOESS Curve Fitting')) + 
  labs(x = 'Number of Principal Components Used in Cross-Validation',y = 'R-squared') +
  scale_colour_manual(name="Legend", values = c("blue",'red'))

which(res_RMSE==min(res_RMSE))
which(res_Rsquared==max(res_Rsquared))
gridExtra::grid.arrange(g1,g2)

```
# ANOVA
```{r}
models = list()
for (i in 1:14){
  PCnames = paste('PC',1:i,sep = '')
  y = analyticData[,c('RIDAGEYR','Race','Gender','BMI',PCnames)] %>% na.omit()
  set.seed(111)
  trainidx = sample(nrow(y),0.7*nrow(y))
  model = lm(BMI ~., data = y, subset = trainidx)
  models = c(models,list(model))
}


do.call(anova,models)

# print(xtable(anova(model)))
```



# Linear regression


```{r}
rm(list =ls())
load(file = 'analyticData.rda')


PCnames = paste('PC',1:8,sep = '')
Y = analyticData[,c('RIDAGEYR','Race','Gender','BMI',PCnames)] %>% na.omit()
y = Y
yTrue = y$BMI
set.seed(111)
trainidx = sample(nrow(y),0.7*nrow(y))
fit2 = lm(BMI ~ .,data = y, subset = trainidx)







PCnames = paste('PC',1:14,sep = '')
Y = analyticData[,c('RIDAGEYR','Race','Gender','BMI',PCnames)] %>% na.omit()
y = Y
yTrue = y$BMI
# y = as.data.frame(lapply(y, function (x) if (is.factor(x)) unclass(x) %>% as.factor() else x))
# set.seed(111)
# trainidx = sample(nrow(y),0.7*nrow(y))


# y$BMI = log(y$BMI +1)



fit = lm(BMI ~ .,data = y, subset = trainidx)


# anova(fit,fit2)
summary(fit)
# summary(fit2)

# library(car)
# avPlots(glm(BMI ~ .,data = y, subset = trainidx),ask = FALSE)
if(F){
vif(fit)
}
# y_cor <- as.data.frame(
#   lapply(y, function (x) if (is.factor(x)) unclass(x) %>% as.numeric  else x))
# res_cor = cor(y_cor)
# library(corrplot)
# corrplot(res_cor,
#          metho = 'color',
#          type = 'upper',
#          order = 'hclust',
#          addCoef.col = "black",
#          diag = F)

# yPred = exp(predict(fit,y[-trainidx,]))-1
yPred = predict(fit,y[-trainidx,])
yTrue = yTrue[-trainidx]
result = cbind(yPred,
               yTrue) %>% as.data.frame() %>% na.omit()

# MSE of yPred and yTrue
# mean((yPred-yTrue)^2)



ggplot(data=result , aes(x = 1:dim(result)[1])) + 
  geom_line(aes(y = yPred),color = 'red') + 
  geom_line(aes(y = yTrue),color = 'blue') +
  labs(x = 'Patients', y = 'BMI',
       title  = "PCs + Age + Race + Gender")


ggplot(data = result) + 
  aes(x = yTrue, y = yPred) +
  geom_point()+
  # geom_smooth(se = TRUE, method = 'lm')+
  geom_smooth(se = TRUE, method =  'auto',color = 'red') +
  labs(y = 'Predicted BMI', x = 'Actual BMI') +
  theme(axis.text=element_text(size=20),
        axis.title=element_text(size=20))

# plot(fit2,which = 1,main = "interaction term added")
plot(fit,which = 1)




```


# Linear regression results

```{r}

plot(yPred, yTrue)

# MSE
mean((result[,1]-result[,2])^2)

# cor
cor(result[,1], result[,2])

summary(fit2)
```



```{r}
# library(pixiedust)
# dust(fit) %>% 
#   sprinkle(cols = c("estimate", "std.error", "statistic"), round = 3) %>%
#       sprinkle(cols = "p.value", fn = quote(pvalString(value))) %>% 
#       sprinkle_colnames("Term", "Coefficient", "SE", "T-statistic", 
#                         "P-value")

```

# Plot Table
```{r message=F}
# library(sjmisc)
# library(sjlabelled)

library(sjPlot)
tab_model(fit,
          ci.hyphen = ", ",
          string.ci = "CI (95%)",
          string.p = "P-Value",
          dv.labels = "Regression Model using the first 14 PCs")
tab_model(fit2,
          ci.hyphen = ", ",
          string.ci = "CI (95%)",
          string.p = "P-Value",
          dv.labels = "Regression Model using the first 8 PCs")

tab_model(fit,fit2,
          ci.hyphen = ", ",
          show.ci = FALSE,
          dv.labels = c("Regression Model using the first 14 PCs",
                        "Regression Model using the first 8 PCs"))
```





# DEEP LEARNING data processing
```{r}
load('analyticData.rda')
library(rnhanesdata)
keep_inx <- exclude_accel(act = PAXINTEN_D, flags = Flags_D)
accel_good_D <- PAXINTEN_D[keep_inx,] 
flags_good_D <- Flags_D[keep_inx,]


MIN_name = grep('MIN',colnames(accel_good_D),value = T)
MINdata = data.frame(SEQN = accel_good_D[,'SEQN'],accel_good_D[,MIN_name] * flags_good_D[,MIN_name]) %>%
  group_by(SEQN) %>% summarise_each(funs(mean)) %>% as.data.frame() %>% na.omit()


SEQN_7days = data.frame(SEQN = accel_good_D[,'SEQN'],accel_good_D[,MIN_name] * flags_good_D[,MIN_name]) %>% group_by(SEQN) %>% summarise(num = n()) %>% filter(num==7) %>% select(SEQN)

MINdata = MINdata %>% filter(SEQN %in% SEQN_7days$SEQN)


MINdata= MINdata %>% 
  inner_join(Covariate_D %>% select(SEQN,Race,Gender,RIDAGEYR,BMI),by = 'SEQN') %>% 
  na.omit() 

# 1440*7 format

MINdata_1440 = data.frame(SEQN = accel_good_D[,'SEQN'],accel_good_D[,MIN_name] * flags_good_D[,MIN_name]) %>% as.data.frame() %>% na.omit()
MINdata_1440 = MINdata_1440 %>% filter(SEQN %in% SEQN_7days$SEQN)
MINdata_1440= MINdata_1440 %>% 
  inner_join(Covariate_D %>% select(SEQN,Race,Gender,RIDAGEYR,BMI),by = 'SEQN') %>% 
  na.omit() 

# 1440*7 = 10080 format

MINdata_10080 = data.frame('SEQN' = accel_good_D[,'SEQN'], 'WEEKDAY' = accel_good_D[,'WEEKDAY'],accel_good_D[,6:dim(accel_good_D)[2]] * flags_good_D[,6:dim(accel_good_D)[2]]) %>% 
  arrange(SEQN,WEEKDAY) %>%
  reshape(idvar = "SEQN", timevar = "WEEKDAY",direction = "wide") %>% na.omit()
MINdata_10080= MINdata_10080 %>% 
  inner_join(Covariate_D %>% select(SEQN,Race,Gender,RIDAGEYR,BMI),by = 'SEQN') %>% 
  na.omit() 

MINdata_10080 = MINdata_10080 %>% filter(SEQN %in% SEQN_7days$SEQN)

save(MINdata,MINdata_1440,MINdata_10080,file='MINdata.rda')
```

# CNN 
```{r}
rm(list = ls())
library(keras)
library(caret)
load('MINdata.rda')


MINdata_1440 = as.data.frame(lapply(MINdata_1440, function (x) if (is.factor(x)) unclass(x) %>% as.numeric else x))

# MINdata_10080 = MINdata_10080 %>% select(-Race,-Gender,-RIDAGEYR)

set.seed(000)
trainIdx = sample(c(TRUE, FALSE), dim(MINdata_1440)[1], replace = TRUE, prob = c(.7, .3))

# y = log(MINdata$BMI + 1)
y = MINdata_1440$BMI
x = MINdata_1440 %>% select(-BMI) %>% select(-SEQN)

ytrain = y[trainIdx]
xtrain = x[trainIdx, ] %>% scale()

mns = attr(xtrain, "scaled:center")
sds = attr(xtrain, "scaled:scale")

xtest = x[!trainIdx, ] %>% scale(center = mns, scale = sds)
ytest = y[!trainIdx]



# we are using univariate time series data so number of feature is 1
# for multivariate data e.g. activity signal on axis 1, axis 2, axis 3, then number of feature is 3 
xtrain = array(xtrain, dim = c(dim(xtrain)[1], dim(xtrain)[2], 1))
xtest = array(xtest, dim = c(dim(xtest)[1], dim(xtest)[2], 1))

model_CNN = keras_model_sequential() %>%
  layer_conv_1d(filters = 2^8, kernel_size = 2,
               input_shape = c(dim(xtrain)[2:3]), activation = "relu") %>%
  layer_max_pooling_1d(pool_size = 2) %>%
  layer_conv_1d(filters = 2^4, kernel_size = 2, activation = "relu") %>%
  layer_max_pooling_1d(pool_size = 2) %>%
  layer_flatten() %>%
  layer_dense(units = 2^4, activation = "relu") %>%
  # layer_dropout(0.5) %>%
  layer_dense(units = 1, activation = "linear")


model_CNN %>% compile(
 loss = "mse",
 optimizer = "adam",
 metrics = list("mean_absolute_error")
)

history = model_CNN %>% fit(
 xtrain,
 ytrain,
 epochs = 20,
 validation_split = 0.2,
 # batch_size = 100,
 verbose = 1
)

yPred = model_CNN %>% predict(xtest)
```

```{r}
model_CNN %>% save_model_hdf5("model_CNN.h5")
model_CNN <- load_model_hdf5("model_CNN.h5")
```

```{r}
plot(yPred, ytest)

mean((yPred-ytest)^2)

cor(yPred,ytest)

summary(model_CNN)
```


# to do 
```{r}
# CNN: separate covariates and time-series and then concatenate


```


# CNN with PC scores


# garbage code



# CNN + LSTM

```{r}
rm(list = ls())
load('MINdata.rda')


MINdata_1440 = as.data.frame(lapply(MINdata_1440, function (x) if (is.factor(x)) unclass(x) %>% as.numeric else x))
set.seed(000)
trainIdx = sample(c(TRUE, FALSE), dim(MINdata_1440)[1], replace = TRUE, prob = c(.7, .3))


y = MINdata_1440$BMI 
x = MINdata_1440 %>% select(-BMI) %>% select(-SEQN)

ytrain = y[trainIdx]
xtrain = x[trainIdx, ] %>% scale()

mns = attr(xtrain, "scaled:center")
sds = attr(xtrain, "scaled:scale")

xtest = x[!trainIdx, ] %>% scale(center = mns, scale = sds)
ytest = y[!trainIdx]


xtrain = array(xtrain, dim = c(dim(xtrain)[1], dim(xtrain)[2], 1))
xtest = array(xtest, dim = c(dim(xtest)[1], dim(xtest)[2], 1))


model_custom = keras_model_sequential() %>%
  layer_conv_1d(filters = 2^8, kernel_size = 2,
               input_shape = c(dim(xtrain)[2:3]), activation = "relu") %>%
  layer_max_pooling_1d(pool_size = 2) %>%
  layer_lstm(units = 2^3) %>%
  layer_dense(1, activation = 'linear')

model_custom %>% compile(
 loss = "mse",
 optimizer = "adam",
 metrics = list("mean_absolute_error")
)

history = model_custom %>% fit(
 xtrain,
 ytrain,
 epochs = 20,
 validation_split = 0.2,
 verbose = 1
)

yPred = model_custom %>% predict(xtest)
```

```{r}
model_custom %>% save_model_hdf5("model_custom.h5")
model_custom <- load_model_hdf5("model_custom.h5")
```

```{r}
plot(yPred, ytest)

mean((yPred-ytest)^2)

cor(yPred,ytest)

summary(model_custom)
```

# LSTM

```{r}
rm(list = ls())
load('MINdata.rda')
MINdata_1440 = as.data.frame(lapply(MINdata_1440, function (x) if (is.factor(x)) unclass(x) %>% as.numeric else x))
set.seed(000)
trainIdx = sample(c(TRUE, FALSE), dim(MINdata_1440)[1], replace = TRUE, prob = c(.7, .3))


y = MINdata_1440$BMI 
x = MINdata_1440 %>% select(-BMI) %>% select(-SEQN)

ytrain = y[trainIdx]
xtrain = x[trainIdx, ] %>% scale()

mns = attr(xtrain, "scaled:center")
sds = attr(xtrain, "scaled:scale")

xtest = x[!trainIdx, ] %>% scale(center = mns, scale = sds)
ytest = y[!trainIdx]


tstep = 1
# we are using univariate time series data so number of feature is 1
# for multivariate data e.g. activity signal on axis 1, axis 2, axis 3, then number of feature is 3
# [samples / batch size, tstep, number of features] 
xtrain = array(xtrain, dim = c(dim(xtrain)[1], tstep, 1))
xtest = array(xtest, dim = c(dim(xtest)[1], tstep, 1))


model_LSTM = keras_model_sequential() %>%
  layer_lstm(input_shape = c(dim(xtrain)[2:3]),
             units = 2^8, activation = 'relu') %>% 
  # layer_lstm(units = 2^4, activation = 'relu') %>%
  layer_dense(2^2) %>%
  # layer_dropout(0.25) %>%
  layer_dense(units = 1, activation = "linear")


model_LSTM %>% compile(
 loss = "mse",
 optimizer = "adam",
 metrics = list("mean_absolute_error")
)

history = model_LSTM %>% fit(
 xtrain,
 ytrain,
 epochs = 10,
 validation_split = 0.2,
 verbose = 1
)

yPred = model_LSTM %>% predict(xtest)
```

```{r}
model_LSTM %>% save_model_hdf5("model_LSTM.h5")
model_LSTM <- load_model_hdf5("model_LSTM.h5")
```

```{r}
plot(yPred, ytest)

mean((yPred-ytest)^2)

cor(yPred,ytest)

summary(model_LSTM)
```


## CNN with separate covariates 
```{r}
library(keras)
library(caret)
load('MINdata.rda')


covariates = MINdata %>% select(Race,Gender,RIDAGEYR)
covariates = as.data.frame(lapply(covariates, function (x) if (is.factor(x)) unclass(x) %>% as.numeric else x))

MINdata = as.data.frame(lapply(MINdata, function (x) if (is.factor(x)) unclass(x) %>% as.numeric else x)) %>% select(-Race,-Gender,-RIDAGEYR)

# MINdata_10080 = MINdata_10080 %>% select(-Race,-Gender,-RIDAGEYR)

set.seed(000)
trainIdx = sample(c(TRUE, FALSE), dim(MINdata)[1], replace = TRUE, prob = c(.7, .3))

# y = log(MINdata$BMI + 1)
y = MINdata$BMI
x = MINdata %>% select(-BMI) %>% select(-SEQN)

ytrain = y[trainIdx]
xtrain = x[trainIdx, ] %>% scale()

mns = attr(xtrain, "scaled:center")
sds = attr(xtrain, "scaled:scale")

xtest = x[!trainIdx, ] %>% scale(center = mns, scale = sds)
ytest = y[!trainIdx]

xtrain = cbind(xtrain,covariates[trainIdx, ]) %>% as.matrix()
# xtrain = lapply(xtrain,as.numeric) %>% as.data.frame()
xtest = cbind(xtest,covariates[!trainIdx,]) %>% as.matrix()
# xtest = lapply(xtest,as.numeric) %>% as.data.frame()

# we are using univariate time series data so number of feature is 1
# for multivariate data e.g. activity signal on axis 1, axis 2, axis 3, then number of feature is 3 
xtrain = reticulate::array_reshape(xtrain, dim = c(nrow(xtrain), dim(xtrain)[2], 1))
xtest = reticulate::array_reshape(xtest, dim = c(nrow(xtest), dim(xtest)[2], 1))

model = keras_model_sequential() %>%
  layer_conv_1d(filters = 2^8, kernel_size = 2,
               input_shape = c(dim(xtrain)[2:3]), activation = "relu") %>%
  layer_max_pooling_1d(pool_size = 2) %>%
  layer_conv_1d(filters = 2^4, kernel_size = 2) %>% 
  layer_flatten() %>%
  layer_dense(units = 2^4, activation = "relu") %>%
  layer_dense(units = 1, activation = "linear")

# summary(model)

model %>% compile(
 loss = "mse",
 optimizer = "adam",
 metrics = list("mean_absolute_error")
)

history = model %>% fit(
 xtrain,
 ytrain,
 epochs = 20,
 validation_split = 0.2,
 verbose = 1,
)

yPred = model %>% predict(xtest)
```



```{r}
# yPred = exp(yPred)-1
# ytest = exp(ytest)-1

summary(model)

plot(yPred, ytest)

mean((yPred-ytest)^2)

cor(yPred,ytest)
```



# try different window size
```{r}
if(F){
err = rep(0,10)
for (d in 1:10){
MINdata_smooth = MINdata[,MIN_name] %>%
  apply(MARGIN = 2, function(i) runmean(i,k=d))
MINdata_smooth = data.frame(SEQN = MINdata$SEQN,MINdata_smooth)
pca_smooth = prcomp(MINdata_smooth %>% select(-SEQN),
                 center = T,
                 scale. = T)

y = data.frame(SEQN= MINdata_smooth$SEQN,pca_smooth$x[,1:5]) %>% 
  left_join(Covariate_D[,c('RIDAGEYR','Race','Gender','BMI','SEQN')],by = 'SEQN')
y = y[,c('RIDAGEYR','Race','Gender','BMI','PC1','PC2','PC3','PC4','PC5')] %>% na.omit()

yTrue = y$BMI

y$BMI = log(y$BMI+1)

fit = lm(BMI ~ .,data = y, subset = trainidx)
summary(fit)

yPred = exp(predict(fit,y[-trainidx,]))-1
yTrue = yTrue[-trainidx]
result = cbind(yPred,
               yTrue) %>% as.data.frame() %>% na.omit()


err[d] = mean((result[,1]-result[,2])^2)
}
err
}
# > err
#  [1] 47.93726 49.89128 50.71071 51.17472 51.32988 51.30029 51.16800 51.16298 51.10647
# [10] 51.17049
```






